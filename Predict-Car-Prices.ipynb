{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%%sh\n",
    "echo '{\"kernel_python_credentials\" : {\"url\": \"http://<EMR master node private IP>:8998/\"}, \"session_configs\": \n",
    "{\"executorMemory\": \"2g\",\"executorCores\": 2,\"numExecutors\":4}}' > ~/.sparkmagic/config.json\n",
    "less ~/.sparkmagic/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CHANGE ME ##\n",
    "input_file_path = \"s3://<bucket>/ml/data/data.csv\"\n",
    "s3_model_bucket='<bucket>'\n",
    "spark_model_location='s3://'+s3_model_bucket+'/models/car_price_prediction_model/'\n",
    "pipeline_name='pipeline1'\n",
    "model_name='model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "data = spark.read.csv(path=input_file_path, header=True, quote='\"', sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Mileage: integer (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Trim: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Cylinder: integer (nullable = true)\n",
      " |-- Liter: double (nullable = true)\n",
      " |-- Doors: integer (nullable = true)\n",
      " |-- Cruise: integer (nullable = true)\n",
      " |-- Sound: integer (nullable = true)\n",
      " |-- Leather: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Trim': StringIndexer_472bacc9a3f556535587, 'Make': StringIndexer_475394bfff5acb0d2fb4, 'Type': StringIndexer_426ca2ebe79e94d86d3d, 'Model': StringIndexer_4f31b5e8d4519f5d6dce}"
     ]
    }
   ],
   "source": [
    "data_test, data_train = data.randomSplit(weights=[0.3, 0.7], seed=10)\n",
    "\n",
    "get_indexer_input = get_indexer_input(data)\n",
    "print (get_indexer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_training(data_train, indexer_input):\n",
    "    x_cols = list(set(data_train.columns) - set(indexer_input.keys() + [\"Price\"]))\n",
    "    str_ind_cols = ['indexed_' + column for column in indexer_input.keys()]\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_tr = Pipeline(stages=indexers)\n",
    "    data_tr = pipeline_tr.fit(data_train).transform(data_train)\n",
    "    assembler = VectorAssembler(inputCols=x_cols, outputCol=\"features\")\n",
    "    gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Price\", stepSize=0.008, maxDepth=5, subsamplingRate=0.75,\n",
    "                       seed=10, maxIter=20, minInstancesPerNode=5, checkpointInterval=100, maxBins=64)\n",
    "    pipeline_training = Pipeline(stages=[assembler, gbt])\n",
    "    model = pipeline_training.fit(data_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_testing(model, data_test, indexer_input):\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_te = Pipeline(stages=indexers)\n",
    "    data_te = pipeline_te.fit(data_test).transform(data_test)\n",
    "    predictions = model.transform(data_te)\n",
    "    predictions.select(\"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_training(data_train, get_indexer_input)\n",
    "model.write().overwrite().save(spark_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the saved model and Test the model.\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml import Pipeline\n",
    "import json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value\n",
    "\n",
    "def model_testing(model, data_test, indexer_input):\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_te = Pipeline(stages=indexers)\n",
    "    data_te = pipeline_te.fit(data_test).transform(data_test)\n",
    "    data_te.show(1,False)\n",
    "    predictions = model.transform(data_test)\n",
    "    predictions.select(\"prediction\").show(10,False)\n",
    "\n",
    "sameModel = PipelineModel.load(path=\"s3://neilawsml/models/CarPrices/\")\n",
    "\n",
    "j={\"Price\":9041.9062544231,\"Mileage\":26191,\"Make\":\"Chevrolet\",\"Model\":\"AVEO\",\"Trim\":\"SVM Sedan 4D\",\"Type\":\"Sedan\",\"Cylinder\":4,\"Liter\"\n",
    ":1.6,\"Doors\":4,\"Cruise\":0,\"Sound\":0,\"Leather\":1}\n",
    "\n",
    "a=[json.dumps(j)]\n",
    "jsonRDD = sc.parallelize(a)\n",
    "df = spark.read.json(jsonRDD)\n",
    "\n",
    "get_indexer_input = get_indexer_input(df)\n",
    "model_testing(sameModel, df, get_indexer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+---------+-----+------------+-----+--------+-----+-----+------+-----+-------+\n",
      "|          Price|Mileage|     Make|Model|        Trim| Type|Cylinder|Liter|Doors|Cruise|Sound|Leather|\n",
      "+---------------+-------+---------+-----+------------+-----+--------+-----+-----+------+-----+-------+\n",
      "|9041.9062544231|  26191|Chevrolet| AVEO|SVM Sedan 4D|Sedan|       4|  1.6|    4|     0|    0|      1|\n",
      "+---------------+-------+---------+-----+------------+-----+--------+-----+-----+------+-----+-------+"
     ]
    }
   ],
   "source": [
    "row_df=data_test.limit(1)\n",
    "row_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Serialize to MLeap Bundle\n",
    "import mleap.pyspark\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "model.serializeToBundle(\"jar:file:/tmp/\"+model_name+\".zip\", model.transform(row_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Object(bucket_name='neilawsml1', key='models/pipeline1/model.zip')"
     ]
    }
   ],
   "source": [
    "#Save the Bundle to S3\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "data = open(\"/tmp/\"+model_name+\".zip\", 'rb')\n",
    "s3.Bucket(s3_model_bucket).put_object(Key='models/'+pipeline_name+'/'+model_name+'.zip', Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.zip\n",
      "cd /home/ec2-user/models/ ; tar -czvf model.tgz model.zip \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "## CHANGE ME ##\n",
    "s3_model_bucket='<bucket>'\n",
    "spark_model_location='s3://'+s3_model_bucket+'/models/car_price_prediction_model/'\n",
    "pipeline_name='pipeline1'\n",
    "model_name='model'\n",
    "home='/home/ec2-user/models/'\n",
    "\n",
    "## Tar Zip the model and save back to S3\n",
    "import boto3,os\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(s3_model_bucket).download_file('models/'+pipeline_name+'/'+model_name+'.zip', home+model_name+'.zip')\n",
    "cmd='cd '+home+' ; tar -czvf '+model_name+'.tgz '+model_name+'.zip '\n",
    "print (cmd)\n",
    "print (os.system(cmd))\n",
    "data = open(home+model_name+'.tgz', 'rb')\n",
    "s3.Bucket(s3_model_bucket).put_object(Key='models/'+pipeline_name+'/'+model_name+'.tgz', Body=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
